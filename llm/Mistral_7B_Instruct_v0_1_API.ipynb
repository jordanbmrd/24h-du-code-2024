{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB6MjE8arBIh",
        "outputId": "ba55fe5b-ea4b-4fc0-e589-ffe764ce7586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.59)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U torch datasets transformers tensorflow langchain playwright sentence_transformers faiss-cpu accelerate\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 trl==0.4.7\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install flask flask-cors pyngrok\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AO3pj53IrDLq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import tokenizers\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  BitsAndBytesConfig,\n",
        "  pipeline\n",
        ")\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow asynchronous events to be processed in Jupyter notebooks.\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cXhIwoIl3Flr"
      },
      "outputs": [],
      "source": [
        "# Fix for Colab\n",
        "# Override the locale.getpreferredencoding function to return \"UTF-8\".\n",
        "# This is a workaround to avoid issues with character encoding in Google Colab, ensuring\n",
        "# that all operations using locale settings default to UTF-8 encoding.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ux2ajAmsrHFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38da614b-befd-4357-e3ce-693c20451a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "# The model identifier for the pre-trained model to be used.\n",
        "model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "\n",
        "# Load the model configuration from the pre-trained model specified by `model_name`.\n",
        "# AutoConfig automatically determines the configuration class appropriate for the pre-trained model.\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        ")\n",
        "\n",
        "# Load the tokenizer associated with the specified model.\n",
        "# The tokenizer is responsible for converting text inputs into a format that the model can understand.\n",
        "# `trust_remote_code=True` allows the loading of custom tokenizers from the hub, which may execute remote code.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# Set the padding token to be the same as the end-of-sequence (eos) token.\n",
        "# This is often done in models that treat the end of a sequence similarly to padding.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Configure the tokenizer to pad sequences on the right (default behavior).\n",
        "# This means that any padding added to a sequence of tokens will be appended at the end.\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2knABHdprIeY"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# use_4bit: Activates 4-bit precision for base model loading.\n",
        "# Using 4-bit precision can significantly reduce the memory footprint of the model,\n",
        "# allowing larger models or higher batch sizes on the same hardware.\n",
        "use_4bit = True\n",
        "\n",
        "# bnb_4bit_compute_dtype: Specifies the data type for computations when using 4-bit base models.\n",
        "# \"float16\" is used here to strike a balance between precision and computational performance,\n",
        "# particularly on GPUs that support this half-precision format effectively.\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# bnb_4bit_quant_type: Defines the quantization type to use for 4-bit models.\n",
        "# Options are 'fp4' for four-bit floating point or 'nf4' for four-bit normalized fixed-point.\n",
        "# \"nf4\" can offer better performance and lower memory usage compared to traditional floating point.\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# use_nested_quant: Enables nested quantization for 4-bit base models. This option allows\n",
        "# double quantization where model weights are first quantized to 4-bit precision and can be\n",
        "# further quantized internally, potentially improving memory efficiency but at a risk of\n",
        "# higher information loss.\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wza-xpxSrKGI"
      },
      "outputs": [],
      "source": [
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "\n",
        "# compute_dtype: Retrieves the data type for computations from the PyTorch library,\n",
        "# which corresponds to the earlier defined `bnb_4bit_compute_dtype` (e.g., float16).\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "# bnb_config: Configuration for the BitsAndBytes library, which handles model quantization.\n",
        "# - load_in_4bit: Whether to load models in 4-bit precision.\n",
        "# - bnb_4bit_quant_type: Specifies the quantization type (e.g., 'nf4').\n",
        "# - bnb_4bit_compute_dtype: Sets the data type for computations in quantized models.\n",
        "# - bnb_4bit_use_double_quant: Enables nested quantization if set to True.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# GPU compatibility check for bfloat16:\n",
        "# If using float16 and 4-bit precision is enabled, check if the GPU supports bfloat16,\n",
        "# which is beneficial for accelerating training. This check is crucial for ensuring\n",
        "# compatibility and performance optimization.\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()  # Retrieves GPU compute capability\n",
        "    if major >= 8:  # Check if the GPU architecture supports bfloat16 (available in SM >= 8.0)\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "68snHuL9rMcw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "57b891eadf1d466aa73ff84900b124fa",
            "f8af3ff896484cd7acfd395f89b6d11e",
            "975c122fd58f45f790ad109281a05667",
            "f17ad9476cb540e0b893859917a9f7cd",
            "82c160977f054369a61b2b66306969fc",
            "be33c98d9cf7435f9a71cd47eca7aa58",
            "ce6d347d045a4693810c4e7317e48c59",
            "e38392cc5cac4946a70d7ee9893b6395",
            "0055cf9fecc04401bbf5d2c6802fced2",
            "e0c62c77c39a4f4cbedc1d289cfd0b97",
            "afeef4b2adf346d4ae2f34fb34c35678"
          ]
        },
        "outputId": "1e2e4ab2-22a4-4606-ffeb-03fbf1c6585c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57b891eadf1d466aa73ff84900b124fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable model parameters: 262410240\n",
            "all model parameters: 3752071168\n",
            "percentage of trainable model parameters: 6.99%\n"
          ]
        }
      ],
      "source": [
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "\n",
        "# Load a pre-trained causal language model from Hugging Face with a quantization configuration.\n",
        "# The quantization configuration is specified by `bnb_config` to optimize model size and performance.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    # Function to count the total and trainable parameters in the model.\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()  # Sum up all elements in parameters.\n",
        "        if param.requires_grad:  # Check if parameters are trainable.\n",
        "            trainable_model_params += param.numel()  # Sum up trainable parameters.\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "# Print the model's parameter stats.\n",
        "print(print_number_of_trainable_model_parameters(model))\n",
        "\n",
        "# Configure and create a text generation pipeline using the loaded model and tokenizer.\n",
        "# This setup uses specific settings to guide the style and constraints of generated text.\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,  # Low temperature results in more predictable text.\n",
        "    repetition_penalty=1.1,  # Penalty reduces repetition in generated text.\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,  # Limit on the number of new tokens generated.\n",
        ")\n",
        "\n",
        "# Wrap the pipeline into a higher level abstraction for easier usage.\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0sGIInmbrSNP"
      },
      "outputs": [],
      "source": [
        "# Create prompt template\n",
        "# Defines a template for prompts to be used in a language model scenario.\n",
        "# This template is specifically designed for a game scenario, including roles and context.\n",
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Ton rôle est de jouer au jeu du loup-garou. Voici un peu de contexte pour t'aider :\n",
        "\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question}\n",
        " [/INST]\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt structure from the template with specified input variables.\n",
        "# This object facilitates the injection of dynamic content ('context' and 'question') into the template.\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "# Create an LLM chain, a sequential model execution pipeline.\n",
        "# It configures how the large language model (LLM) should process the prompt structure.\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)\n",
        "\n",
        "# Uncomment the lines below to run the LLM chain with actual 'context' and 'question' values and print the result.\n",
        "# This part of the code is used to execute the model with specific input and get the generated response.\n",
        "# result = llm_chain.run({\"context\": context, \"question\": question})\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f-blTTpSbLCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffaf6fe-1f93-49cc-994d-6bd763277eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://09aa-34-145-109-152.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Define a route at '/generate' that accepts POST requests.\n",
        "# This endpoint will be used to generate responses based on provided context and question.\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_text():\n",
        "    # Retrieve JSON data from the POST request.\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Extract 'context' and 'question' from the data, defaulting to an empty string if not found.\n",
        "    context = data.get('context', '')\n",
        "    question = data.get('question', '')\n",
        "\n",
        "    # Run the large language model chain with the provided context and question to generate text.\n",
        "    result = llm_chain.run({\"context\": context, \"question\": question})\n",
        "\n",
        "    # Prepare the response containing the last part of the result.\n",
        "    response = {\"result\": \"\\n\".join(result.splitlines()[-1:])}\n",
        "    # Return the result as a JSON response.\n",
        "    return jsonify(response)\n",
        "\n",
        "# Main entry point of the application\n",
        "if __name__ == '__main__':\n",
        "    port = 5000  # Define the port on which to run the Flask application.\n",
        "\n",
        "    # Authenticate and open an ngrok tunnel to the specified port.\n",
        "    # This allows the Flask app to be accessible over the internet via the ngrok tunnel.\n",
        "    ngrok.set_auth_token(userdata.get('NGROK_TOKEN'))\n",
        "    ngrok_tunnel = ngrok.connect(port)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    # Run the Flask application on the defined port.\n",
        "    app.run(port=port)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57b891eadf1d466aa73ff84900b124fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8af3ff896484cd7acfd395f89b6d11e",
              "IPY_MODEL_975c122fd58f45f790ad109281a05667",
              "IPY_MODEL_f17ad9476cb540e0b893859917a9f7cd"
            ],
            "layout": "IPY_MODEL_82c160977f054369a61b2b66306969fc"
          }
        },
        "f8af3ff896484cd7acfd395f89b6d11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be33c98d9cf7435f9a71cd47eca7aa58",
            "placeholder": "​",
            "style": "IPY_MODEL_ce6d347d045a4693810c4e7317e48c59",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "975c122fd58f45f790ad109281a05667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e38392cc5cac4946a70d7ee9893b6395",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0055cf9fecc04401bbf5d2c6802fced2",
            "value": 2
          }
        },
        "f17ad9476cb540e0b893859917a9f7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c62c77c39a4f4cbedc1d289cfd0b97",
            "placeholder": "​",
            "style": "IPY_MODEL_afeef4b2adf346d4ae2f34fb34c35678",
            "value": " 2/2 [01:22&lt;00:00, 37.97s/it]"
          }
        },
        "82c160977f054369a61b2b66306969fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be33c98d9cf7435f9a71cd47eca7aa58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce6d347d045a4693810c4e7317e48c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e38392cc5cac4946a70d7ee9893b6395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0055cf9fecc04401bbf5d2c6802fced2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0c62c77c39a4f4cbedc1d289cfd0b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afeef4b2adf346d4ae2f34fb34c35678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}